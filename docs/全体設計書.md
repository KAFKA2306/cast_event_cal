# VRChatイベントカレンダー 全体設計書

## 1. システムアーキテクチャ

本システムは、データ収集から配信までの一連の処理を自動化するパイプラインとして設計します。全体構成は次の4つの主要レイヤーに分割されます。

```
[データ収集層] → [処理・分析層] → [統合・検証層] → [配信層]
```

### 1.1 全体構成図

```
┌───────────────────┐     ┌───────────────────┐     ┌───────────────────┐     ┌───────────────────┐
│   データ収集層    │     │   処理・分析層    │     │   統合・検証層    │     │     配信層      │
│                   │     │                   │     │                   │     │                   │
│ ・Playwright スクレイパー │ ⟶ │ ・テキスト正規化  │ ⟶ │ ・データ統合      │ ⟶ │ ・Web UI生成    │
│ ・リスト情報収集  │     │ ・イベント情報抽出│     │ ・重複排除        │     │ ・ICS/CSV出力   │
│ ・外部API連携     │     │ ・AIによる強化    │     │ ・スキーマ検証    │     │ ・JSON API生成  │
└───────────────────┘     └───────────────────┘     └───────────────────┘     └───────────────────┘
         ↑                         ↑                         ↑                         ↑
         │                         │                         │                         │
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                                   共通基盤コンポーネント                                 │
│                                                                                         │
│  ・設定管理  ・ロギング  ・エラーハンドリング  ・データストレージ  ・処理スケジューラ    │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

**アーキテクチャ概要:**

*   **データ収集層**: Playwrightを活用し、X.com (旧Twitter) から堅牢かつ効率的にイベント関連情報を収集します。リスト情報や将来的な外部APIからのデータ取得も担当します。
*   **処理・分析層**: 収集した生データを正規化し、イベント情報を抽出します。必要に応じてGemini APIなどのAIを活用し、情報の精度向上や補完を行います。
*   **統合・検証層**: 複数のソースから収集・処理されたデータを統合し、重複を排除します。定義されたデータスキーマに基づき検証を行い、定期イベントの展開も行います。
*   **配信層**: 検証済みのイベントデータを、Web UI、Google Calendar用CSV、iCalendar (.ics) 形式、VRChatワールド向けJSON APIなど、各プラットフォームに適した形式で出力・配信します。
*   **共通基盤コンポーネント**: 設定管理、ロギング、エラーハンドリングなど、システム全体で共通して利用される機能を提供します。

## 2. ディレクトリ構造

プロジェクト全体のファイルとディレクトリの構成案です。

```
vrc-event-calendar/
├── config/                     # 設定ファイル群
│   ├── main_config.yaml       # 主要なシステム設定
│   ├── logging_config.yaml    # ロギングに関する設定
│   └── scraping_targets.yaml  # スクレイピング対象 (クエリ、リストURL等) の定義
│
├── src/                        # Pythonソースコード
│   ├── data_collection/       # データ収集関連モジュール
│   │   ├── playwright_scraper.py # Playwrightを用いたX.comスクレイパー
│   │   ├── list_collector.py    # X.comリスト情報収集
│   │   └── base_collector.py    # 収集モジュールの基底クラス (オプション)
│   │
│   ├── data_processing/       # データ処理・分析関連モジュール
│   │   ├── text_normalizer.py   # テキスト正規化・クリーニング
│   │   ├── event_info_extractor.py # イベント情報抽出ロジック
│   │   └── ai_event_enhancer.py   # AI (Gemini) による情報強化
│   │
│   ├── data_integration/      # データ統合・検証関連モジュール
│   │   ├── event_deduplicator.py  # イベント重複検出・統合
│   │   ├── schema_validator.py    # データスキーマ検証
│   │   └── recurring_event_handler.py # 定期イベント処理・展開
│   │
│   ├── data_publishing/       # データ配信関連モジュール
│   │   ├── calendar_file_generator.py # CSV/ICSカレンダーファイル生成
│   │   ├── json_api_generator.py   # 各種JSON API生成
│   │   └── web_content_updater.py  # Web UI用コンテンツ更新
│   │
│   ├── common/                # システム共通コンポーネント
│   │   ├── configuration_manager.py # 設定ファイル読み込み・管理
│   │   ├── logger_setup.py        # ロギング機能の初期化・提供
│   │   ├── error_handling.py      # 例外処理・エラー通知
│   │   ├── data_storage_handler.py # データファイル入出力管理
│   │   └── utility_functions.py   # 汎用的な補助関数
│   │
│   └── models/                # データ構造定義
│       ├── event_data_model.py    # イベント情報データクラス/モデル
│       ├── organizer_data_model.py # 主催者情報データクラス/モデル
│       └── data_schemas.py        # JSONスキーマ定義
│
├── pipelines/                  # 各処理パイプラインの定義・実行スクリプト
│   ├── daily_collection_pipeline.py   # 日次データ収集パイプライン
│   ├── event_processing_pipeline.py  # イベント情報処理パイプライン
│   └── data_publishing_pipeline.py   # データ配信パイプライン
│
├── data/                       # 各処理段階のデータファイル
│   ├── raw_scraped_data/      # スクレイピング直後の生データ
│   ├── processed_events/      # 情報抽出・処理後のデータ
│   ├── validated_events/      # 検証済みのイベントデータ
│   └── published_outputs/     # 公開用ファイル (CSV, ICS, JSON)
│
├── web_frontend/               # Web UI関連ファイル (GitHub Pages等で公開)
│   ├── assets/                 # CSS, JavaScript, 画像ファイル
│   │   ├── css/
│   │   ├── js/
│   │   └── images/
│   ├── templates/              # HTMLテンプレート (静的サイトジェネレータ用など)
│   └── api_data/               # 公開用APIデータ (JSONファイルなど)
│
├── tests/                      # 自動テストコード
│   ├── unit_tests/             # 各モジュールのユニットテスト
│   ├── integration_tests/      # 複数モジュール連携の統合テスト
│   └── test_fixtures/          # テスト用データファイル
│
├── docs/                       # プロジェクトドキュメント
│   ├── api_specification/      # API仕様書
│   └── user_guide/             # 利用者向けガイド
│
├── vrc_assets/                 # VRChatワールド用アセット
│   ├── udonsharp_scripts/      # UdonSharpスクリプト
│   └── unity_prefabs/          # Unityプレハブ
│
├── .github/                    # GitHub Actions 関連設定
│   └── workflows/              # CI/CDワークフロー定義ファイル
│
├── requirements.txt            # Python依存パッケージリスト (Playwright, ics含む)
├── main_executor.py            # パイプライン実行のメインエントリーポイント
└── README.md                   # プロジェクト概要説明ファイル
```

## 3. コアモジュール設計

主要なモジュールのクラスと役割について記述します。（コードは含みません）

### 3.1 データ収集モジュール (`src/data_collection/`)

#### 3.1.1 `playwright_scraper.py`

*   **クラス名**: `PlaywrightTwitterScraper`
*   **役割**: Playwrightを使用し、X.comからイベント関連ツイートをスクレイピングする。
*   **主要メソッド**:
    *   `__init__(self, scraping_config, logger_instance)`: 設定情報とロガーで初期化。
    *   `initialize_browser(self)`: Playwrightブラウザインスタンスを初期化・設定（ヘッドレスモード、ユーザーエージェント偽装など）。
    *   `login_to_twitter(self, username, password)`: X.comへのログイン処理を実行。2段階認証にも対応可能な堅牢な実装を想定。
    *   `search_tweets(self, search_query, max_tweets_to_fetch, max_days_to_look_back)`: 指定されたクエリでツイートを検索し、指定件数または期間内のツイートを取得。Playwrightの**自動待機機能**を最大限活用し、不安定な `sleep` 呼び出しを避ける。無限スクロールに対応。
    *   `extract_tweet_details_from_element(self, tweet_element)`: Playwrightの**ロケータAPI** (CSSセレクタ、XPath、テキスト内容など) を活用し、個々のツイート要素から必要な情報（ツイートID, 本文, 日時, URL, 画像URL等）を**柔軟かつ安定的に**抽出する。
    *   `save_raw_data(self, collected_tweets, output_directory)`: 収集した生データを指定されたディレクトリにJSONまたはCSV形式で保存。
    *   `enable_tracing(self, trace_output_path)`: デバッグ用にPlaywrightの**トレース機能**を有効化し、実行証跡を記録する。
    *   `close_browser(self)`: ブラウザインスタンスを適切に閉じる。
*   **考慮事項**: レート制限への対応、エラーハンドリング、スクレイピングブロック回避策の実装。

#### 3.1.2 `list_collector.py`

*   **クラス名**: `TwitterListInfoCollector`
*   **役割**: 指定されたX.comリストからメンバー情報（プロフィールURLなど）や、各メンバーのプロフィール詳細（自己紹介文、固定ツイートなど）を収集する。Playwrightを内部で使用。
*   **主要メソッド**:
    *   `get_list_members(self, list_url, max_members_to_fetch)`: リストページにアクセスし、メンバーのプロフィールURLを取得。
    *   `fetch_profile_details(self, profile_url)`: 個別のプロフィールページにアクセスし、詳細情報を抽出。ロケータAPIを活用。
    *   `save_profile_data(self, profile_data_list, output_directory)`: 収集したプロフィール情報を保存。

### 3.2 データ処理モジュール (`src/data_processing/`)

#### 3.2.1 `event_info_extractor.py`

*   **クラス名**: `EventInformationExtractor`
*   **役割**: 正規化されたテキストデータ（ツイート本文、プロフィール等）から、正規表現やキーワードマッチングを用いてイベント情報（日時、タイトル、主催者、場所等）を構造化データとして抽出する。
*   **主要メソッド**:
    *   `__init__(self, extraction_rules_config)`: 抽出ルール（正規表現パターン等）を含む設定で初期化。
    *   `extract_event_details(self, text_content, associated_metadata)`: メインの抽出処理。テキストと関連メタデータ（投稿者名など）からイベント情報を抽出。
    *   `extract_datetime(self, text)`: 多様な形式の日付・時刻表現を解析し、標準形式に変換。
    *   `extract_event_title(self, text)`: イベントタイトルを抽出。
    *   `extract_organizer_info(self, text, author_name)`: 主催者名を抽出。
    *   `extract_location_details(self, text)`: 開催場所（ワールド名、インスタンスタイプ）を抽出。
    *   `normalize_extracted_data(self, extracted_info)`: 抽出した情報をデータモデルに沿った形式に整形。

#### 3.2.2 `ai_event_enhancer.py`

*   **クラス名**: `GeminiEventDataEnhancer`
*   **役割**: Google Gemini APIを利用して、抽出されたイベント情報の精度向上、欠損情報の補完、曖昧さの解消などを行う。
*   **主要メソッド**:
    *   `__init__(self, api_key, enhancement_config)`: APIキーと設定で初期化。
    *   `enhance_single_event(self, event_data_dict)`: 個別のイベントデータを受け取り、Gemini APIに適切なプロンプトを送信して情報を強化・検証する。
    *   `batch_enhance_events(self, list_of_event_data, batch_size)`: 複数のイベントデータをバッチ処理し、API呼び出し効率を向上させる。
    *   `build_prompt_for_extraction(self, raw_text)`: 生テキストから情報を抽出するためのプロンプトを生成。
    *   `build_prompt_for_validation(self, structured_event_data)`: 既存の構造化データの内容を検証・補完するためのプロンプトを生成。

### 3.3 データ統合モジュール (`src/data_integration/`)

#### 3.3.1 `event_deduplicator.py`

*   **クラス名**: `EventDuplicateHandler`
*   **役割**: 異なるソースから収集された、あるいは類似した内容のイベント情報の重複を検出し、適切に統合（マージ）する。
*   **主要メソッド**:
    *   `__init__(self, deduplication_config)`: 重複判定の閾値や重み付けの設定で初期化。
    *   `calculate_similarity_score(self, event_1, event_2)`: 2つのイベント間の類似度（タイトル、日時、主催者など）を計算。
    *   `find_duplicate_groups(self, list_of_events)`: イベントリスト全体から重複している可能性のあるイベントのグループを見つける。
    *   `merge_duplicate_events(self, duplicate_event_group)`: 重複グループ内の情報を統合し、一つの代表イベントデータを生成。情報の信頼性や新しさを考慮。
    *   `process_deduplication(self, list_of_events)`: イベントリスト全体の重複排除処理を実行。

#### 3.3.2 `recurring_event_handler.py`

*   **クラス名**: `RecurringEventHandler`
*   **役割**: 定期的に開催されるイベント（毎週、隔週、月次など）のパターンを検出し、未来の開催予定を自動生成（展開）する。
*   **主要メソッド**:
    *   `detect_recurring_patterns(self, historical_event_list)`: 過去のイベントデータから定期開催パターンを特定。
    *   `generate_future_occurrences(self, recurring_pattern_info, projection_end_date)`: 特定されたパターンに基づき、指定された期間までの未来のイベントインスタンスを生成。
    *   `expand_recurring_events(self, current_event_list, detected_patterns)`: 定期イベントを展開し、既存のイベントリストと統合。

### 3.4 データ配信モジュール (`src/data_publishing/`)

#### 3.4.1 `calendar_file_generator.py`

*   **クラス名**: `CalendarFileGenerator`
*   **役割**: 処理・検証済みのイベントデータを、Google Calendar互換のCSV形式および標準的なiCalendar (.ics) 形式のファイルとして出力する。
*   **主要メソッド**:
    *   `__init__(self, export_config)`: 出力設定で初期化。
    *   `generate_google_calendar_csv(self, list_of_events, output_csv_path)`: イベントリストをGoogle Calendarがインポート可能なCSV形式に変換して保存。
    *   `generate_icalendar_file(self, list_of_events, output_ics_path)`: イベントリストをiCalendar (.ics) 形式に変換して保存。この際、**`ics` ライブラリ**のような、直感的で強力なAPIを持つライブラリの利用を推奨。これにより、繰り返しルール（RRULE）の設定なども容易に行える。
    *   `format_event_for_calendar_output(self, event_data_dict)`: 個々のイベントデータを各カレンダー形式（CSVの行、ICSのVEVENT）に必要なフォーマットに変換。

#### 3.4.2 `json_api_generator.py`

*   **クラス名**: `JsonApiGenerator`
*   **役割**: イベントデータを、Web UIやVRChatワールド内から利用しやすいJSON形式のAPIデータとして生成・出力する。
*   **主要メソッド**:
    *   `generate_lightweight_vrc_api(self, list_of_events, output_json_path)`: VRChat (UdonSharp) での利用を想定し、必要最低限のフィールドに絞った軽量なJSONファイルを生成。
    *   `generate_full_web_api(self, list_of_events, output_json_path)`: Web UIでの表示に必要な詳細情報を含む、完全なJSONファイルを生成。
    *   `optimize_data_for_vrchat(self, event_data_dict)`: VRChat向けにデータ構造やフィールド名を最適化。

## 4. メインエントリーポイント (`main_executor.py`)

*   **ファイル名**: `main_executor.py`
*   **役割**: コマンドライン引数を受け取り、設定ファイルを読み込み、指定されたモード（収集、処理、配信、または全て）に応じて各パイプラインクラスを順次実行する、システム全体の起動スクリプト。
*   **主要関数**:
    *   `parse_command_line_arguments()`: 実行モードや設定ファイルパスなどのコマンドライン引数を解析。
    *   `main_execution_flow()`: メインの実行ロジック。設定を読み込み、ロガーを初期化し、引数に応じて各パイプラインの `run()` メソッドを呼び出す。全体の実行ステータス（成功/失敗）を管理。

## 5. 設定ファイル (`config/`)

*   **`main_config.yaml`**: システム全体の動作、APIキー（環境変数からの参照推奨）、各モジュールの挙動（閾値、有効/無効フラグなど）、入出力パスなどを定義。
*   **`logging_config.yaml`**: ログレベル、フォーマット、出力先（ファイル、コンソール）などを定義。
*   **`scraping_targets.yaml`**: X.comの検索クエリ、監視対象のリストURL、その他のデータソース定義など、データ収集のターゲット情報を分離して管理。

## 6. 自動化・CI/CD (`.github/workflows/`)

*   **ワークフローファイル例**: `update_calendar_data.yml`
*   **役割**: GitHub Actionsを利用し、定期的（例: 8時間ごと）または手動トリガーで、データ収集から配信までの全パイプラインを自動実行する。
*   **主要ステップ**:
    1.  リポジトリのチェックアウト
    2.  Python環境のセットアップ
    3.  `requirements.txt` に基づく依存パッケージ（Playwright, ics など）のインストール
    4.  Playwright用ブラウザバイナリのインストール (`playwright install`)
    5.  環境変数（APIキー、ログイン情報）の設定
    6.  `main_executor.py --mode all` を実行
    7.  変更があった場合（`data/published_outputs/` や `web_frontend/api_data/` 内のファイル）、変更内容をコミット＆プッシュしてGitHub Pages等にデプロイ。

## 7. 拡張性と今後の発展

*   **データソース追加**: `src/data_collection/` に新しいコレクタークラスを追加し、パイプラインに組み込むことで容易に対応可能。
*   **出力形式追加**: `src/data_publishing/` に新しいジェネレータクラスを追加。
*   **AIモデル変更**: `src/data_processing/ai_event_enhancer.py` を修正または新しいクラスを追加して対応。
*   **Web UI**: `web_frontend/` を独立したフレームワーク（Vue, Reactなど）で構築することも可能。

